---
layout: default
---


<section style="text-align: justify">
	<h1> Visual Domain Adaptation Challenge </h1>
	<h2> (VisDA-2022) </h2>
</br>

<!-- [<a href = "#news">News</a>]
[<a href = "#overview">Overview</a>]
[<a href = "#data">Data</a>]
[<a href = "https://github.com/VisionLearningGroup/visda-2019-public">Data and Code</a>]
[<a href= "#prizes">Prizes</a>]
[<a href = "#evaluation">Evaluation</a>]
[<a href = "#rules">Rules</a>]
[<a href = "#faq">FAQ</a>] 
[<a href = "https://sites.google.com/view/task-cv2019">TASK-CV Workshop</a>]
[<a href = "#organizers">Organizers</a>]
[<a href = "#sponsors">Sponsors</a>]
</br> </br> </nr>

<a name = "news"></a>

<h2 class="section-title"> Workshop </h2>

<p> Please joint our <a href="https://groups.google.com/g/visda-2021-participants">[mailing list]</a> for official announcements!</p>

We are happy to announce that the live breakout session of the VisDA-2021 NeurIPS workshop took place on Tuesday, December 7th 2021 at 20:00 GMT. Presentations, recordings, and techincal reports can be found below.

</br> </br> <b style="font-size: 20px;"> Leaderboard </b> </br> </br>

<table style="width:100%" class="table">
  <tr class="active">
    <th width='3%'>#</th>
    <th width='25%'>Team Name</th>
    <th>Affiliation</th> 
    <th width='4%'>ACC / AUROC</th>
  </tr>
  <tr>
    <td> 1 </td>
	  <td>babychick </br> <a href="https://www.youtube.com/watch?v=6S5woHLSqpQ&t=2124s">[video]</a> <a href="https://ai.bu.edu/visda-2021/assets/pdf/Burhan_Slides.pdf">[slides]</a> <a href="https://ai.bu.edu/visda-2021/assets/pdf/Burhan_Report.pdf">[pdf]</a> </td>
<td>Shirley Robotics </br> <i>Burhan Ul Tayyab, Nicholas Chua</i> </td> 
    <td>56.29 / 69.79</td>
  </tr>
  <tr>
    <td> 2 </td>
	  <td> chamorajg </br> <a href="https://www.youtube.com/watch?v=6S5woHLSqpQ&t=1372s">[video]</a> <a href="https://ai.bu.edu/visda-2021/assets/pdf/Chandramouli_Slides.pdf">[slides]</a> <a href="https://ai.bu.edu/visda-2021/assets/pdf/Chandramouli_Report.pdf">[pdf]</a> </td>
<td>- </br> <i>Chandramouli Rajagopalan</i> </td> 
    <td>48.49 / 76.86</td>
  </tr>
  <tr>
    <td> 3 </td>
	  <td> liaohaojin </br> <a href="https://www.youtube.com/watch?v=6S5woHLSqpQ&t=304s">[video]</a> <a href="https://arxiv.org/pdf/2110.14240.pdf">[pdf]</a> </td>
	  <td>Beijing University of Posts and Telecommunications </br> <i>Haojin Liao, Xiaolin Song, Sicheng Zhao, Shanghang Zhang, Xiangyu Yue, Xingxu Yao, Yueming Zhang, Tengfei Xing, Pengfei Xu, Qiang Wang</i> </td> 
    <td>48.49 / 70.8</td>
  </tr>
</table>

<b style="font-size: 20px;"> Recordings of Invited Talks </b> </br> </br>
<ul>
	<li> <i>Haojin Liao</i>: <b>3rd Place</b> <a href="https://www.youtube.com/watch?v=6S5woHLSqpQ&t=304s">[video]</a> </li>
	<li> <i>Chandramouli Rajagopalan</i>: <b>2nd Place</b> <a href="https://www.youtube.com/watch?v=6S5woHLSqpQ&t=1372s">[video]</a> </li>
	<li> <i>Burhan Ul Tayyab</i>: <b>1st Place</b> <a href="https://www.youtube.com/watch?v=6S5woHLSqpQ&t=2124s">[video]</a> <a href="https://https://ai.bu.edu/visda-2021/assets/pdf/Burhan_Slides.pdf">[slides]</a> <a href="https://ai.bu.edu/visda-2021/assets/pdf/Burhan_Report.pdf">[pdf]</a> </li> -->
<!-- <li> "Not All Networks Are Born Equal for Robustness" by <i>Cihang Xie</i> <a href="https://www.youtube.com/watch?v=6S5woHLSqpQ&t=3616s">[video]</a> </li>
<li> "Natural Corruption Robustness: Corruptions, Augmentations and Representations" by <i>Saining Xie</i> <a href="https://www.youtube.com/watch?v=6S5woHLSqpQ&t=4898s">[video]</a> </li>
<li> "Challenges in Deep Learning: Applications to Real-world Ecological Datasets" by <i>Zhongqi Miao</i> <a href="https://www.youtube.com/watch?v=6S5woHLSqpQ&t=6260s">[video]</a> </li>
</ul>

</br></br> -->

<h2 class="section-title"> Overview </h2>
<a name = "overview"></a>

<p> 
  Welcome to the Visual Domain Adaptation 2022 Challenge! This year, our challenge brings domain adaptation closer to real-world applications, as we propose the task of semantic segmentation for industrial waste sorting. It is the 6th edition of the challenge and we look forward to participation from a large and growing NeurIPS community with a breadth of backgrounds. 
</p>

<p>
  In industrial waste sorting, it is impossible to collect a gold standard dataset that fully represents the task, since the visual appearance of the waste stream as well as its content changes overtime and depends on the specific location, season, machinery in use, and many other factors, all of which introduce a natural domain shift between any source/training and target distributions. Therefore, this year, we challenge the computer vision community to come up with effective solutions for the domain adaptation problem in this real-world application. Our challenge consists of a large-scale synthetic SynthWaste dataset as well as the real ZeroWaste dataset as source domains, and ZeroWasteV2, the real dataset collected at a different location and season, as a target domain. Additionally, we provide SynthWaste-aug, a version of SynthWaste augmented with instance-level style transfer to further increase the diversity of the synthetic dataset. The goal of this challenge is to answer the question: can synthetic data improve performance on this task and help adapt to changing data distributions? We invite the teams to help answer this question and facilitate research aimed at solving the automated waste sorting problem.
</p>


<figure>
<img class="img-responsive" style="display:block;margin-left:auto;margin-right:auto" src="assets/images/zerowaste_domains.png"/>
</figure>

</br>
</br>

<a name = "announcements"></a>
<h2 class="section-title"> Announcements </h2>
<p>
  <li> <b> June 6th: </b> Register your teams <a href="https://forms.gle/R7c8zJQbX92qW8X2A">[here]</a>.
  <li> <b> June 24th: </b> Training data and starter code available <a href="https://github.com/dbash/visda2022-org">[here]</a>. Evaluation server coming soon.
  <li> <b> June 30th: </b> Evaluation server and interface is up on <a href="https://eval.ai/web/challenges/challenge-page/1806/overview">eval.ai</a>. Find submission instructions <a href="https://github.com/dbash/visda2022-org#submit-on-evalai">[here]</a>.
    <!-- <ul>
    <li> <b> June 23rd: </b> the official devkit and with data urls are released on <a href="https://github.com/VisionLearningGroup/visda21-dev">[github]</a>. </li>
    <li> <b> July 7th: </b> the evaluation server and the leaderboard are up on <a href="https://competitions.codalab.org/competitions/33396">[codalab]</a>
    <li> <b> July 26th: </b> the technical report describing the challenge is avaliable on <a href="https://arxiv.org/abs/2107.11011">[arxiv]</a>.
    <li> <b> July 28th: </b> the <a href="#prizes">[prize fund]</a> is finalized
    <li> <b> Aug 10th: </b> the <a href="#rules">[rules]</a> regarding ensembling and energy efficiency are updated.
    <li> <b> Sept 30th: </b> <a href="#data">[test data]</a> and an example submission are released, test leaderboard is live.
  </ul> -->
  </p>

</br>
</br>

<a name = "prizes"></a>
<h2 class="section-title"> Prizes </h2>

<p> The top three teams will receive pre-paid VISA gift cards: $2000 for the 1st place, $500 for 2nd and 3rd. </p> </br>

</br>
</br>

<a name = "rules"></a>
<h2 class="section-title"> Rules </h2>
 
<ul>
<li>Participants from both academic and industrial institutions are welcome to participate in our challenge. The participating teams agree to publish the full code that allows reproducibility in case of a victory (top 3 solutions). The list of team members must be provided during registration, and cannot be changed throughout the competition. Each individual can only participate in one team and should provide the institution / corporate email and a phone number at registration. </li>
<li>Teams may use any publicly available and appropriately licensed data to train their models in addition to the ones provided by the organizers. Supervised training on the validation set of the target domain is not allowed in this competition.</li>
<li>Models can be adapted using the unsupervised dataset from the target domain without access to the labels. For example, solutions that involve supervised training on the validation set or manual labeling of the target domain will be disqualified.</li>
<li>The choice of segmentation backbone greatly affects the domain adaptation results. To allow fair comparison, participants will be asked to submit the following results: source-only predictions of a backbone model trained only on data from the source domains, as well as the domain adaptation results of the model trained on source and unlabeled target data. </li>
<li>The winning solutions will be determined based on the test mIoU of the adapted model on target data. In an unlikely event of a tie, mean pixel accuracy will be used to break the tie. </li>
<li>To encourage fair competition regardless of affiliation and compute capabilities, we limit the overall model size to 300 millions of parameters. If multiple distinct models are used, the total number of parameters will be counted according to the number of forward passes of the same input batch. For example, if the same input example is passed to the model that has 150 millions of parameters twice, it will count as 300 millions of parameters in total.</li>
<li>Reproducibility is the responsibility of the winning teams. Top-3 winning solutions must submit the full code along with a complete list of instructions / script on how to reproduce their results, ideally with the exact random seeds used to get the best result. If the organizing committee determines that the submitted code runs with errors or does not yield results comparable to those in the final leaderboard and the team is not willing to cooperate, it will be disqualified, and the winning place will go to the next team in the leaderboard. </li>
<li>Energy efficiency: Teams must report the total training time of the submitted model which should be reasonable (which we define as not exceeding 100 GPU days of V100 (16GB version) but contact us if unsure). Energy efficient solutions will be highlighted even if they do not finish in the top three. </li>
</ul>

</br>
</br>


<a name = "sponsors"></a>
<h2 class="section-title"> Sponsors </h2>

<img src="assets/images/boston_univ_rgb.gif"  height="100">

</br>
</br>


<a name = "organizers"></a>
<h2 class="section-title"> Organizers </h2>

Dina Bashkirova (BU), Piotr Teterwak (BU), Samarth Mishra (BU), Donghyun Kim (BU), Diala Lteif (BU), Rachel Lai (BU), Fadi Alladkani (WPI), James Akl (WPI), Berk Calli (WPI), Vitaly Ablavsky (UW), Sarah Adel Bargal (BU), Kate Saenko (BU & MIT-IBM Watson AI)

</br>
</br>



</div><!--//summary-->
</section><!--//section-->
